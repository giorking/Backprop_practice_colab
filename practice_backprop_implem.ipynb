{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcpaQGlvotdv6FOxVZwxnq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/giorking/Backprop_practice_colab/blob/main/practice_backprop_implem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from abc import ABC, abstractmethod\n",
        "import multiprocessing as mp\n",
        "import threading\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "class Operation(ABC):\n",
        "    @abstractmethod\n",
        "    def forward(self, *inputs):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def backward(self, dout):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward_torch(self, *inputs):\n",
        "        pass\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4spZgrq9Fsa",
        "outputId": "6b15664e-6eec-4f3d-c015-da55d3bf4ead"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(func, inputs, epsilon=1e-5):\n",
        "    gradients = []\n",
        "    for i, input in enumerate(inputs):\n",
        "        grad = np.zeros_like(input)\n",
        "        it = np.nditer(input, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            idx = it.multi_index\n",
        "\n",
        "            original_value = input[idx]\n",
        "\n",
        "            # f(x + epsilon)\n",
        "            input[idx] = original_value + epsilon\n",
        "            plus_epsilon = func(*inputs)\n",
        "\n",
        "            # f(x - epsilon)\n",
        "            input[idx] = original_value - epsilon\n",
        "            minus_epsilon = func(*inputs)\n",
        "\n",
        "            # Numerical gradient\n",
        "            grad[idx] = (plus_epsilon - minus_epsilon) / (2 * epsilon)\n",
        "\n",
        "            # Restore original value\n",
        "            input[idx] = original_value\n",
        "            it.iternext()\n",
        "\n",
        "        gradients.append(grad)\n",
        "    return gradients\n",
        "\n",
        "def numerical_gradient_loss(func, X, y, epsilon=1e-5):\n",
        "    gradients = []\n",
        "    for i, input in enumerate(X):\n",
        "        grad = np.zeros_like(input)\n",
        "        it = np.nditer(input, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            idx = it.multi_index\n",
        "\n",
        "            original_value = input[idx]\n",
        "\n",
        "            # f(x + epsilon)\n",
        "            input[idx] = original_value + epsilon\n",
        "            plus_epsilon = func(X, y)\n",
        "\n",
        "            # f(x - epsilon)\n",
        "            input[idx] = original_value - epsilon\n",
        "            minus_epsilon = func(X, y)\n",
        "\n",
        "            # Numerical gradient\n",
        "            grad[idx] = (plus_epsilon - minus_epsilon) / (2 * epsilon)\n",
        "\n",
        "            # Restore original value\n",
        "            input[idx] = original_value\n",
        "            it.iternext()\n",
        "\n",
        "        gradients.append(grad)\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "8lGMlmnGJsrl"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dot_mt(X, W, num_threads=4):\n",
        "    if W.ndim == 2:\n",
        "        result = np.zeros((X.shape[0], W.shape[1]))\n",
        "    else:\n",
        "        result = np.zeros((X.shape[0], ))\n",
        "    rows_per_thread = X.shape[0] // num_threads\n",
        "\n",
        "    def worker(A, B, result, start_row, end_row):\n",
        "        if W.ndim == 2:\n",
        "            result[start_row:end_row, :] = np.dot(A[start_row:end_row, :], B)\n",
        "        else:\n",
        "            result[start_row:end_row] = np.dot(A[start_row:end_row, :], B)\n",
        "\n",
        "    threads = []\n",
        "    for i in range(num_threads):\n",
        "        start_row = i * rows_per_thread\n",
        "        end_row = (i + 1) * rows_per_thread if i != num_threads - 1 else X.shape[0]\n",
        "\n",
        "        thread = threading.Thread(target=worker, args=(X, W, result, start_row, end_row))\n",
        "        threads.append(thread)\n",
        "        thread.start()\n",
        "\n",
        "    for thread in threads:\n",
        "        thread.join()\n",
        "\n",
        "    return result\n",
        "\n",
        "def dot_mp(X, W, num_threads=4):\n",
        "    if W.ndim == 2:\n",
        "        result = mp.Array('d', X.shape[0] * W.shape[1])\n",
        "    else:\n",
        "        result = mp.Array('d', X.shape[0])\n",
        "\n",
        "    if W.ndim == 2:\n",
        "        result_np = np.frombuffer(result.get_obj()).reshape(X.shape[0], W.shape[1])\n",
        "    else:\n",
        "        result_np = np.frombuffer(result.get_obj()).reshape(X.shape[0], )\n",
        "    rows_per_process = X.shape[0] // num_threads\n",
        "\n",
        "    def worker(A, B, result, start_row, end_row):\n",
        "        if W.ndim == 2:\n",
        "            result[start_row:end_row, :] = np.dot(A[start_row:end_row, :], B)\n",
        "        else:\n",
        "            result[start_row:end_row, ] = np.dot(A[start_row:end_row, :], B)\n",
        "\n",
        "    processes = []\n",
        "    for i in range(num_threads):\n",
        "        start_row = i * rows_per_process\n",
        "        end_row = (i + 1) * rows_per_process if i != num_threads - 1 else X.shape[0]\n",
        "\n",
        "        process = mp.Process(target=worker, args=(X, W, result_np, start_row, end_row))\n",
        "        processes.append(process)\n",
        "        process.start()\n",
        "\n",
        "    for process in processes:\n",
        "        process.join()\n",
        "\n",
        "    return result_np\n",
        "\n"
      ],
      "metadata": {
        "id": "p3F44fvidlmy"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_operation(operation_class, *inputs):\n",
        "    print(f\"Testing {operation_class.__name__}...\", \"  numpy vs pytorch\")\n",
        "    operation = operation_class()\n",
        "\n",
        "    out = operation.forward(*inputs)\n",
        "    dout = np.random.randn(*out.shape)\n",
        "    grads = operation.backward(dout)\n",
        "\n",
        "    inputs_torch = [torch.tensor(inp, requires_grad=True, dtype=torch.float32) for inp in inputs]\n",
        "    out_torch = operation.forward_torch(*inputs_torch)\n",
        "    dout_torch = torch.tensor(dout, dtype=torch.float32)\n",
        "    out_torch.backward(dout_torch)\n",
        "\n",
        "    grads_torch = [inp.grad.detach().numpy() for inp in inputs_torch]\n",
        "\n",
        "\n",
        "    assert len(grads) == len(grads_torch), \"Number of gradients does not match!\"\n",
        "\n",
        "\n",
        "    for grad, grad_torch in zip(grads, grads_torch):\n",
        "        print(f\"Difference in gradients: {np.linalg.norm(grad - grad_torch)}\")\n",
        "        assert np.allclose(grad, grad_torch, atol=1e-5), \"Gradients do not match!\"\n",
        "\n",
        "def test_loss_operation(operation_class, X, y):\n",
        "    print(f\"Testing {operation_class.__name__}...\", \"  numpy vs pytorch\")\n",
        "\n",
        "    operation = operation_class()\n",
        "\n",
        "\n",
        "    out = operation.forward(X, y)\n",
        "    dout = 1.0\n",
        "    grad = operation.backward(dout)\n",
        "\n",
        "    X_torch = torch.tensor(X, requires_grad=True, dtype=torch.float32)\n",
        "    y_torch = torch.tensor(y, dtype=torch.long if operation_class == CrossEntropy else torch.float32)\n",
        "    out_torch = operation.forward_torch(X_torch, y_torch)\n",
        "    out_torch.backward()\n",
        "\n",
        "    # 获取 PyTorch 的梯度\n",
        "    grad_torch = X_torch.grad.detach().numpy()\n",
        "\n",
        "    assert len(grad) == len(grad_torch), \"Number of gradients does not match!\"\n",
        "\n",
        "    print(f\"Difference in gradients: {np.linalg.norm(grad - grad_torch)}\")\n",
        "    assert np.allclose(grad, grad_torch, atol=1e-5), \"Gradients do not match!\"\n",
        "\n",
        "def test_loss_numerical_gradient(operation_class, X, y, epsilon=1e-5):\n",
        "    print(f\"Testing {operation_class.__name__}...\", \"  backprop vs numerical\")\n",
        "\n",
        "    operation = operation_class()\n",
        "\n",
        "\n",
        "    output = operation.forward(X, y)\n",
        "    dout = 1.0\n",
        "\n",
        "\n",
        "    def func(X, y):\n",
        "        return np.sum(operation.forward(X, y))\n",
        "\n",
        "\n",
        "    numerical_grads = numerical_gradient_loss(func, X, y, epsilon)\n",
        "\n",
        "\n",
        "    analytical_grads = operation.backward(dout)\n",
        "\n",
        "\n",
        "    assert len(numerical_grads) == len(analytical_grads), \"Number of gradients does not match!\"\n",
        "\n",
        "\n",
        "    for num_grad, ana_grad in zip(numerical_grads, analytical_grads):\n",
        "        difference = np.linalg.norm(num_grad - ana_grad)\n",
        "        print(f\"Difference in gradients: {difference}\")\n",
        "        assert np.allclose(num_grad, ana_grad, atol=1e-5), f\"Gradients do not match! Difference: {difference}\"\n",
        "\n",
        "def test_numerical_gradient(operation_class, *inputs, epsilon=1e-1):\n",
        "    print(f\"Testing {operation_class.__name__}...\", \"  backprop vs numerical\")\n",
        "\n",
        "    operation = operation_class()\n",
        "\n",
        "\n",
        "    output = operation.forward(*inputs)\n",
        "\n",
        "\n",
        "    def func(*inputs):\n",
        "        return np.sum(operation.forward(*inputs))\n",
        "\n",
        "    numerical_grads = numerical_gradient(func, list(inputs), epsilon)\n",
        "\n",
        "\n",
        "    dout = np.ones_like(output)\n",
        "    analytical_grads = operation.backward(dout)\n",
        "\n",
        "\n",
        "    assert len(numerical_grads) == len(analytical_grads), \"Number of gradients does not match!\"\n",
        "\n",
        "\n",
        "    for num_grad, ana_grad in zip(numerical_grads, analytical_grads):\n",
        "        difference = np.linalg.norm(num_grad - ana_grad)\n",
        "        print(f\"Difference in gradients: {difference}\")\n",
        "        assert np.allclose(num_grad, ana_grad, atol=1e-5), f\"Gradients do not match! Difference: {difference},  {num_grad},   {ana_grad}\"\n"
      ],
      "metadata": {
        "id": "N0sixfUIkO2F"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Conv2D(Operation):\n",
        "    def __init__(self, stride=1, padding=0):\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "    def forward(self, X, W, b):\n",
        "        self.X, self.W, self.b = X, W, b\n",
        "        # Get dimensions\n",
        "        N, C, H, W = self.X.shape\n",
        "        F, _, HH, WW = self.W.shape\n",
        "        out_height = (H - HH + 2 * self.padding) // self.stride + 1\n",
        "        out_width = (W - WW + 2 * self.padding) // self.stride + 1\n",
        "\n",
        "        # Pad the input\n",
        "        X_padded = np.pad(self.X, ((0,), (0,), (self.padding,), (self.padding,)), mode='constant')\n",
        "\n",
        "        # Initialize output\n",
        "        out = np.zeros((N, F, out_height, out_width))\n",
        "\n",
        "        for n in range(N):\n",
        "            for f in range(F):\n",
        "                for i in range(0, out_height):\n",
        "                    for j in range(0, out_width):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + HH\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + WW\n",
        "\n",
        "                        out[n, f, i, j] = np.sum(X_padded[n, :, h_start:h_end, w_start:w_end] * self.W[f, :, :, :]) + self.b[f]\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        X, W, b = self.X, self.W, self.b\n",
        "        N, C, H, W = self.X.shape\n",
        "        F, _, HH, WW = self.W.shape\n",
        "        out_height = (H - HH + 2 * self.padding) // self.stride + 1\n",
        "        out_width = (W - WW + 2 * self.padding) // self.stride + 1\n",
        "\n",
        "        # Pad the input and gradients\n",
        "        X_padded = np.pad(self.X, ((0,), (0,), (self.padding,), (self.padding,)), mode='constant')\n",
        "        dX_padded = np.pad(np.zeros_like(X), ((0,), (0,), (self.padding,), (self.padding,)), mode='constant')\n",
        "        dW = np.zeros_like(self.W)\n",
        "        db = np.zeros_like(self.b)\n",
        "\n",
        "        for n in range(N):\n",
        "            for f in range(F):\n",
        "                for i in range(out_height):\n",
        "                    for j in range(out_width):\n",
        "                        h_start = i * self.stride\n",
        "                        h_end = h_start + HH\n",
        "                        w_start = j * self.stride\n",
        "                        w_end = w_start + WW\n",
        "\n",
        "                        db[f] += dout[n, f, i, j]\n",
        "                        dW[f] += X_padded[n, :, h_start:h_end, w_start:w_end] * dout[n, f, i, j]\n",
        "                        dX_padded[n, :, h_start:h_end, w_start:w_end] += self.W[f] * dout[n, f, i, j]\n",
        "\n",
        "        # Remove padding from dX\n",
        "        if self.padding > 0:\n",
        "            dX = dX_padded[:, :, self.padding:-self.padding, self.padding:-self.padding]\n",
        "        else:\n",
        "            dX = dX_padded\n",
        "\n",
        "        return dX, dW, db\n",
        "\n",
        "    def forward_torch(self, X, W, b):\n",
        "        return F.conv2d(X, W, bias=b, stride=self.stride, padding=self.padding)\n",
        "\n",
        "class MatMul(Operation):\n",
        "    def forward(self, X, W):\n",
        "        self.X, self.W = X, W\n",
        "        return dot.dot(X, W)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dX = dot.dot(dout, self.W.T)\n",
        "        dW = dot.dot(self.X.T, dout)\n",
        "        return dX, dW\n",
        "\n",
        "    def forward_torch(self, X, W):\n",
        "        return torch.matmul(X, W)\n",
        "\n",
        "class FullyConnected(Operation):\n",
        "    def forward(self, X, W, b):\n",
        "        self.X, self.W, self.b = X, W, b\n",
        "        return dot.dot(X, W) + b\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dX = dot.dot(dout, self.W.T)\n",
        "        dW = dot.dot(self.X.T, dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "        return dX, dW, db\n",
        "\n",
        "    def forward_torch(self, X, W, b):\n",
        "        return torch.matmul(X, W) + b\n",
        "\n",
        "class Softmax(Operation):\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        e_X = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        self.Y = e_X / e_X.sum(axis=1, keepdims=True)\n",
        "        return self.Y\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dX = np.zeros_like(self.Y)\n",
        "        for i, (y, dy) in enumerate(zip(self.Y, dout)):\n",
        "            y = y.reshape(-1, 1)\n",
        "            jacobian = np.diagflat(y) - dot.dot(y, y.T)\n",
        "            dX[i] = dot.dot(jacobian, dy)\n",
        "        return [dX]\n",
        "\n",
        "    def forward_torch(self, X):\n",
        "        return torch.softmax(X, dim=1)\n",
        "\n",
        "\n",
        "class CrossEntropy(Operation):\n",
        "    def forward(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        m = y.shape[0]\n",
        "        p = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
        "        p = p / np.sum(p, axis=1, keepdims=True)\n",
        "        log_likelihood = -np.log(p[range(m), y])\n",
        "        self.loss = np.sum(log_likelihood) / m\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        m = self.y.shape[0]\n",
        "        grad = np.exp(self.X - np.max(self.X, axis=1, keepdims=True))\n",
        "        grad = grad / np.sum(grad, axis=1, keepdims=True)\n",
        "        grad[range(m), self.y] -= 1\n",
        "        grad = grad / m\n",
        "        return grad * dout\n",
        "\n",
        "    def forward_torch(self, X, y):\n",
        "        return torch.nn.functional.cross_entropy(X, y.type(torch.LongTensor))\n",
        "\n",
        "class MSE(Operation):\n",
        "    def forward(self, X, y):\n",
        "        self.X, self.y = X, y\n",
        "        self.loss = np.mean((X - y) ** 2)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dX = (2 * (self.X - self.y) / self.X.size) * dout\n",
        "        return dX\n",
        "\n",
        "    def forward_torch(self, X, y):\n",
        "        return torch.nn.functional.mse_loss(X, y)\n",
        "\n",
        "\n",
        "class ReLU(Operation):\n",
        "    def forward(self, X):\n",
        "        self.X = X\n",
        "        return np.maximum(0, X)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dX = dout.copy()\n",
        "        dX[self.X <= 0] = 0\n",
        "        return [dX]\n",
        "\n",
        "    def forward_torch(self, X):\n",
        "        return torch.nn.functional.relu(X)\n",
        "\n",
        "class Outer(Operation):\n",
        "    def forward(self, a, b):\n",
        "        self.a, self.b = a, b\n",
        "        return np.outer(a, b)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        da = dot.dot(dout, self.b)\n",
        "        db = dot.dot(dout.T, self.a)\n",
        "        return da, db\n",
        "\n",
        "    def forward_torch(self, a, b):\n",
        "        return torch.ger(a, b)\n",
        "\n",
        "class Custom(Operation):\n",
        "    def forward(self, W, X, Y, b):\n",
        "        self.W, self.X, self.Y, self.b = W, X, Y, b\n",
        "        return dot.dot(dot.dot(W, X), Y) + b\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dW = dot.dot(dout, dot.dot(self.X, self.Y).T)\n",
        "        dX = dot.dot(dot.dot(self.W.T, dout), self.Y.T)\n",
        "        dY = dot.dot(dot.dot(self.X.T, self.W.T), dout)\n",
        "        db = np.sum(dout, axis=0)\n",
        "        return dW, dX, dY, db\n",
        "\n",
        "    def forward_torch(self, W, X, Y, b):\n",
        "        return torch.matmul(torch.matmul(W, X), Y) + b"
      ],
      "metadata": {
        "id": "L6v7_lL0d7Sh"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dot:\n",
        "    def __init__(self, method='np'):\n",
        "        self.method = method\n",
        "\n",
        "    def dot(self, X, W, num_threads=4):\n",
        "        if self.method == 'np':\n",
        "            return np.dot(X, W)\n",
        "        elif self.method == 'mt':\n",
        "            return dot_mt(X, W, num_threads)\n",
        "        elif self.method == 'mp':\n",
        "            return dot_mp(X, W, num_threads)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown method: {self.method}\")\n",
        "\n",
        "DOT_METHOD = 'np'\n",
        "dot = dot(method=DOT_METHOD)\n",
        "\n",
        "\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(4, 5)\n",
        "W = np.random.randn(5, 3)\n",
        "b = np.random.randn(3)\n",
        "y_regression = np.random.randn(4, 5)\n",
        "y_classification = np.random.randint(0, 3, 4)\n",
        "\n",
        "#### Testing MatMul ####\n",
        "test_operation(MatMul, X, W)\n",
        "test_numerical_gradient(MatMul, X, W)\n",
        "\n",
        "#### Testing FullyConnected ####\n",
        "test_operation(FullyConnected, X, W, b)\n",
        "test_numerical_gradient(FullyConnected, X, W, b)\n",
        "\n",
        "\n",
        "#### Testing Softmax ####\n",
        "test_operation(Softmax, X)\n",
        "test_numerical_gradient(Softmax, X)\n",
        "\n",
        "#### Testing ReLU ####\n",
        "test_operation(ReLU, X)\n",
        "test_numerical_gradient(ReLU, X)\n",
        "\n",
        "#### Testing Outer ####\n",
        "test_operation(Outer, X[0], W[:, 0])\n",
        "test_numerical_gradient(Outer, X[0], W[:, 0])\n",
        "\n",
        "#### Testing CrossEntropy ####\n",
        "test_loss_operation(CrossEntropy, X, y_classification)\n",
        "test_loss_numerical_gradient(CrossEntropy, X, y_classification)\n",
        "\n",
        "#### Testing MSE ####\n",
        "test_loss_operation(MSE, X, y_regression)\n",
        "test_loss_numerical_gradient(MSE, X, y_regression)\n",
        "\n",
        "\n",
        "#### Testing Conv2D ####\n",
        "X = np.random.randn(1, 3, 5, 5).astype(np.float32)\n",
        "W = np.random.randn(2, 3, 3, 3).astype(np.float32)\n",
        "b = np.random.randn(2).astype(np.float32)\n",
        "test_operation(Conv2D, X, W, b)\n",
        "test_numerical_gradient(Conv2D, X, W, b)\n",
        "\n",
        "#### Custom OP ####\n",
        "# Z = WXY + b\n",
        "W = np.random.randn(4, 5)\n",
        "X = np.random.randn(5, 3)\n",
        "Y = np.random.randn(3, 2)\n",
        "b = np.random.randn(2)\n",
        "test_operation(Custom, W, X, Y, b)\n",
        "test_numerical_gradient(Custom, W, X, Y, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uydf7xG8d8Si",
        "outputId": "fb697b94-802f-417b-aa49-8fc9bb802db1"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing MatMul...   numpy vs pytorch\n",
            "Difference in gradients: 3.4171469138303537e-07\n",
            "Difference in gradients: 3.588504285654481e-07\n",
            "Testing MatMul...   backprop vs numerical\n",
            "Difference in gradients: 1.6221418424348217e-14\n",
            "Difference in gradients: 1.0621069129169845e-14\n",
            "Testing FullyConnected...   numpy vs pytorch\n",
            "Difference in gradients: 5.832149985818068e-07\n",
            "Difference in gradients: 5.212060067835709e-07\n",
            "Difference in gradients: 2.464024254619843e-07\n",
            "Testing FullyConnected...   backprop vs numerical\n",
            "Difference in gradients: 2.7328630811257136e-14\n",
            "Difference in gradients: 2.4237468006307348e-14\n",
            "Difference in gradients: 1.4540107379219425e-14\n",
            "Testing Softmax...   numpy vs pytorch\n",
            "Difference in gradients: 5.350668399524569e-08\n",
            "Testing Softmax...   backprop vs numerical\n",
            "Difference in gradients: 9.391328366696729e-15\n",
            "Testing ReLU...   numpy vs pytorch\n",
            "Difference in gradients: 1.279301609422155e-07\n",
            "Testing ReLU...   backprop vs numerical\n",
            "Difference in gradients: 1.432144669219779e-14\n",
            "Testing Outer...   numpy vs pytorch\n",
            "Difference in gradients: 4.46388413565186e-07\n",
            "Difference in gradients: 3.770075578917267e-07\n",
            "Testing Outer...   backprop vs numerical\n",
            "Difference in gradients: 1.6968624882578708e-14\n",
            "Difference in gradients: 1.888293221827313e-14\n",
            "Testing CrossEntropy...   numpy vs pytorch\n",
            "Difference in gradients: 1.6070756891930707e-08\n",
            "Testing CrossEntropy...   backprop vs numerical\n",
            "Difference in gradients: 1.3003901490914166e-11\n",
            "Difference in gradients: 2.120572865176289e-11\n",
            "Difference in gradients: 2.3531091647404052e-11\n",
            "Difference in gradients: 2.5526998946443665e-11\n",
            "Testing MSE...   numpy vs pytorch\n",
            "Difference in gradients: 3.2710140024072026e-08\n",
            "Testing MSE...   backprop vs numerical\n",
            "Difference in gradients: 2.1421418409669625e-11\n",
            "Difference in gradients: 3.344986114090638e-11\n",
            "Difference in gradients: 3.003210566662323e-11\n",
            "Difference in gradients: 1.497721438638516e-11\n",
            "Testing Conv2D...   numpy vs pytorch\n",
            "Difference in gradients: 1.8482246559869964e-06\n",
            "Difference in gradients: 1.1699072501869523e-06\n",
            "Difference in gradients: 2.384185791015625e-07\n",
            "Testing Conv2D...   backprop vs numerical\n",
            "Difference in gradients: 3.352807470946573e-05\n",
            "Difference in gradients: 3.8894941098988056e-05\n",
            "Difference in gradients: 9.725607924337965e-06\n",
            "Testing Custom...   numpy vs pytorch\n",
            "Difference in gradients: 7.04485006568842e-07\n",
            "Difference in gradients: 4.1475774036307156e-07\n",
            "Difference in gradients: 6.753442359238244e-07\n",
            "Difference in gradients: 2.2161976460810516e-08\n",
            "Testing Custom...   backprop vs numerical\n",
            "Difference in gradients: 3.283258178980923e-14\n",
            "Difference in gradients: 4.626022934898002e-14\n",
            "Difference in gradients: 2.2548189581758933e-14\n",
            "Difference in gradients: 5.0242958677880805e-15\n"
          ]
        }
      ]
    }
  ]
}